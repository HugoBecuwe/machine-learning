{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://supaerodatascience.github.io/machine-learning/\">https://supaerodatascience.github.io/machine-learning/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Boosting</div>\n",
    "\n",
    "Boosting is a procedure that combines several \"weak\" predictors into a powerful \"committee\". It belongs to the family of committee-based or ensemble methods in Machine Learning.\n",
    "\n",
    "The most popular Boosting algorithm is AdaBoost.M1 (Freund & Schapire, 1997).\n",
    "\n",
    "A motivation for Boosting:<br>\n",
    "*AdaBoost with trees is the best off-the-shelf classifier in the world.* (Breiman 1998)<br>\n",
    "It is not so true anymore today but still accurate enough in practice.\n",
    "\n",
    "For recent criticism on that statement, see:<br>\n",
    "**Random classification noise defeats all convex potential boosters.**<br>\n",
    "P.H. Long, R.A. Servedio, *Machine Learning*, **78**(3), 287-304, (2008).\n",
    "\n",
    "In this notebook we take a very practical approach. For a more thorough and rigorous presentation, see (for instance) the reference below.<br>\n",
    "**The boosting approach to machine learning: An overview.**<br>\n",
    "R. E. Schapire. *MSRI workshop on Nonlinear Estimation and Classification*, (2002).\n",
    "\n",
    "1. [Probably Approximately Correct learning](#sec1)\n",
    "2. [AdaBoost](#sec2)\n",
    "3. [Implementing AdaBoost with trees](#sec3)\n",
    "4. [AdaBoost in scikit-learn](#sec4)\n",
    "5. [Gradient Boosting](#sec5)\n",
    "6. [Examples](#sec6)\n",
    "    1. [Spam or Ham?](#sec6-1)\n",
    "    2. [NIST](#sec6-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Probably Approximately Correct learning\n",
    "\n",
    "Probably approximately correct (PAC) learning theory helps analyze whether and under what conditions a learning algorithm will probably output an approximately correct classifier.\n",
    "\n",
    "**\"Approximately correct\"**. A classifier $h(x)=y$ belonging to some family of fonctions $\\mathcal{H}$ and generated by the considered learning algorithm is approximately correct if its error over the distribution $p$ of inputs (its generalization error) is bounded by some $\\varepsilon$ (with $0\\leq \\varepsilon \\leq \\frac{1}{2}$), that is $\\mathbb{E}_{x\\sim p} \\left( h\\left(x\\right)\\neq\\left(x\\in c \\right) \\right) = \\int\\limits_X I_{h\\left(x\\right)\\neq\\left(x\\in c\\right)} \\textrm{d}p(x) \\leq \\varepsilon$.\n",
    "\n",
    "**\"Probably\"**. If, given enough data, the algorithm will output such an approximately correct classifier with probability $1-\\delta$ (with $0 \\leq \\delta \\leq \\frac{1}{2}$), we call that algorithm *Probably Approximately Correct* (PAC).\n",
    "\n",
    "In other words, an algorithm generating classifiers $h(x)=y$ is said to be a *Probably Approximately Correct* (PAC) learner (or *PAC-strong* learner) if,<br>\n",
    "for all $\\left\\{\\begin{array}{l}\n",
    "p\\textrm{ a distribution over } X\\\\\n",
    "\\varepsilon \\in ]0;0.5[\\\\\n",
    "\\delta \\in ]0;0.5[\\\\\n",
    "c \\in \\mathcal{P}(X) \\textrm{ a subset of X defining a classification task}\n",
    "\\end{array}\\right.$, given enough training data,\n",
    "$$\\mathbb{P}\\left(\\int\\limits_X I_{h\\left(x\\right)\\neq\\left(x\\in c\\right)} \\textrm{d}p(x)\\leq \\varepsilon\\right) \\geq 1-\\delta.$$\n",
    "\n",
    "Knowing that a target concept $h(x)=y$ is PAC-learnable allows one to lower bound the sample size $m$ necessary to probably learn an approximately correct classifier:\n",
    "$$m\\geq \\frac{1}{\\varepsilon} \\left( \\log|\\mathcal{H}| + \\log\\left(\\frac{1}{\\delta}\\right) \\right)$$\n",
    "\n",
    "This lower bound implies that:\n",
    "- As the precision requirement gets harder (as $\\varepsilon$ gets closer to zero) the necessary sample size increases.\n",
    "- Similarly, if the number of possible classifiers ($|\\mathcal{H}|$) increases, the sample size required to disambiguate between two classifiers with good probability increases.\n",
    "- Finally, as the desired probability of correctness increases (as $\\delta$ gets closer to zero), the required sample size increases as well.\n",
    "\n",
    "Want more details?<br>\n",
    "**A theory of the learnable.**<br>\n",
    "L.G. Valiant. *Communications of the ACM*, **27**(11):1134-1142, (1984).<br>\n",
    "**The strength of weak learnability.**<br>\n",
    "R.E. Schapire. *Machine Learning*, **5**(2), 197-227, (1990).<br>\n",
    "[Wikipedia page on sample complexity](https://en.wikipedia.org/wiki/Sample_complexity)\n",
    "\n",
    "An algorithm generating classifiers $h(x)=y$ is said to be a *weak* (or *PAC-weak*) learner if, for any training set, it performs better than a random guessing on the training data.\n",
    "This means that its generalization error is strictly below than 0.5 with high probability, formally:\n",
    "$$\\exists \\gamma > 0, \\mathbb{P}\\left(\\int\\limits_X I_{h\\left(x\\right)\\neq\\left(x\\in c\\right)} \\textrm{d}p(x)\\leq 0.5 - \\gamma \\right) \\geq 1-\\delta.$$\n",
    "This is almost the same property as PAC-strong, but restricted to the existence of an $\\epsilon$ smaller than 0.5 rather than for any value of $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a> 2. AdaBoost\n",
    "\n",
    "AdaBoost constructs a PAC-strong classifier $f$ as a linear combination of weak classifiers $h_t(x)$:\n",
    "$$f(x) = \\sum\\limits_{t=1}^T \\alpha_t h_t(x)$$\n",
    "\n",
    "The algorithm:\n",
    "<div class=\"alert alert-success\">\n",
    "Given $\\left\\{\\left(x_i,y_i\\right)\\right\\}_{1 \\leq i \\leq q}, x_i \\in X, y_i \\in \\{-1;1\\}$.<br>\n",
    "Initialize weights $D_1(i) = \\frac{1}{q}$<br>\n",
    "For $t=1$ to $T$:\n",
    "<ul>\n",
    "<li> Find $h_t = \\arg\\min\\limits_{h\\in\\mathcal{H}} \\sum\\limits_{i=1}^q D_t(i) I(y_i\\neq h(x_i))$\n",
    "<li> If $\\epsilon_t = \\sum\\limits_{i=1}^q D_t(i) I(y_i\\neq h_t(x_i)) \\geq 1/2$ then stop\n",
    "<li> Set $\\alpha_t = \\frac{1}{2} \\log\\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)$\n",
    "<li> Update\n",
    "$$D_{t+1}(i) = \\frac{D_t(i) e^{-\\alpha_t y_i h_t(x_i)}}{Z_t}$$\n",
    "Where $Z_t$ is a normalisation factor.\n",
    "</ul>\n",
    "\n",
    "Return the classifier\n",
    "$$H(x) = sign\\left(\\sum\\limits_{t=1}^T \\alpha_t h_t(x) \\right)$$\n",
    "</div>\n",
    "\n",
    "Consequently, AdaBoost learns a sequence of classifiers, by performing *iterative reweighting* over the training data.\n",
    "\n",
    "$$D_{t+1}(i) = \\frac{D_t(i) e^{-\\alpha_t y_i h_t(x_i)}}{Z_t}$$\n",
    "\n",
    "- Increase the weight of incorrectly classified samples\n",
    "- Decrease the weight of correctly classified samples\n",
    "- Memory effect: a sample misclassified several times has a large $D(i)$\n",
    "- $h_t$ focusses on samples that were misclassified by $h_0, \\ldots, h_{t-1}$\n",
    "\n",
    "Property of the training error:\n",
    "\n",
    "$$\\frac{1}{q} \\sum\\limits_{i=1}^q I\\left(H(x_i)\\neq y_i\\right) \\leq \\prod\\limits_{t=1}^T Z_t$$\n",
    "\n",
    "- To minimize training error at each step $t$, minimize this upper bound.<br>\n",
    "$\\rightarrow$ This is where $\\alpha_t = \\frac{1}{2} \\log\\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)$ comes from.\n",
    "- This is actually equivalent to maximizing a (geometrical) margin.\n",
    "\n",
    "Many variants of AdaBoost:\n",
    "- Binary classification AdaBoost.M1, AdaBoost.M2, ...\n",
    "- Multiclass AdaBoost.MH,\n",
    "- Regression AdaBoost.R,\n",
    "\n",
    "And other Boosting algorithms (BrownBoost, AnyBoost...).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "AdaBoost is a meta-algorithm: it \"boosts\" a weak classification algorithm into a committee that is a strong classifier.\n",
    "<ul>\n",
    "<li> AdaBoost maximizes margin\n",
    "<li> Very simple to implement\n",
    "<li> Can be seen as a feature selection algorithm\n",
    "<li> In practice, AdaBoost often avoids overfitting.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\"></a> 3. Implementing AdaBoost with trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X1, y1 = datasets.make_gaussian_quantiles(cov=2.,\n",
    "                                 n_samples=300, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X2, y2 = datasets.make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n",
    "                                 n_samples=700, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, - y2 + 1))\n",
    "y = 2*y-1\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "Xtest,X = np.split(X,[400])\n",
    "ytest,y = np.split(y,[400])\n",
    "\n",
    "Xblue = X[y==-1]\n",
    "Xred = X[y==1]\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(Xblue[:,0],Xblue[:,1],c='b',s=20)\n",
    "_=plt.scatter(Xred[:,0],Xred[:,1],c='r',s=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grow a first tree and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: dot: command not found\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'dt1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/default_code/lib/python3.9/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/default_code/lib/python3.9/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/default_code/lib/python3.9/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_mimebundle_\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mmimetype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mimetype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malways_both\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmimetype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/default_code/lib/python3.9/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1304\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'dt1.png'"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: 'dt1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/default_code/lib/python3.9/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/default_code/lib/python3.9/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/default_code/lib/python3.9/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_png_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FMT_PNG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_jpeg_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/default_code/lib/python3.9/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[0;34m(self, always_both)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0mb64_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m             raise FileNotFoundError(\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[1;32m   1304\u001b[0m         \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: 'dt1.png'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from os import system\n",
    "from IPython.display import Image\n",
    "\n",
    "dt1 = tree.DecisionTreeClassifier(criterion='entropy',max_depth=3)\n",
    "dt1.fit(X,y)\n",
    "\n",
    "def disp_tree(filename, treename):\n",
    "    dotfile = open(filename+'.dot', 'w')\n",
    "    tree.export_graphviz(treename, \n",
    "                         out_file = dotfile,\n",
    "                         filled=True,\n",
    "                         rounded=True,  \n",
    "                         special_characters=True)\n",
    "    dotfile.close()\n",
    "    system(\"dot -Tpng \"+filename+\".dot -o \"+filename+\".png\")\n",
    "    return Image(filename+'.png')\n",
    "\n",
    "disp_tree('dt1',dt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to plot the tree's decision boundary. Let's take this occasion to plot the misclassified training points (in cyan and magenta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(t, X, y, fig_size=(7,7)):\n",
    "    plot_step = 0.02\n",
    "    x0_min, x0_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, plot_step), np.arange(x1_min, x1_max, plot_step))\n",
    "    yypred = t.predict(np.c_[xx0.ravel(),xx1.ravel()])\n",
    "    yypred = yypred.reshape(xx0.shape)\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.contourf(xx0, xx1, yypred, cmap=plt.cm.Paired)\n",
    "    y_pred = t.predict(X)\n",
    "    Xblue_good = X[np.equal(y,-1)*np.equal(y,y_pred)]\n",
    "    Xblue_bad  = X[np.equal(y,-1)*np.not_equal(y,y_pred)]\n",
    "    Xred_good  = X[np.equal(y,1)*np.equal(y,y_pred)]\n",
    "    Xred_bad   = X[np.equal(y,1)*np.not_equal(y,y_pred)]\n",
    "    plt.scatter(Xblue_good[:,0],Xblue_good[:,1],c='b',s=20)\n",
    "    plt.scatter(Xblue_bad[:,0],Xblue_bad[:,1],c='c',marker='v',s=20)\n",
    "    plt.scatter(Xred_good[:,0],Xred_good[:,1],c='r',s=20)\n",
    "    plt.scatter(Xred_bad[:,0],Xred_bad[:,1],c='m',marker='v',s=20)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(dt1, X, y)\n",
    "print(\"Training error: %g\"%(1-dt1.score(X,y)))\n",
    "print(\"Testing error:  %g\"%(1-dt1.score(Xtest,ytest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement our AdaBoost algorithm, we will need a function that evaluates the majority vote of a given set of trees (a forest) and another function that plots the forest's decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_predict(f, weights, X):\n",
    "    N = len(f)\n",
    "    votes = np.zeros((X.shape[0],N))\n",
    "    for i in range(N):\n",
    "        votes[:,i] = weights[i]*f[i].predict(X)\n",
    "    pred = np.sum(votes,axis=1)\n",
    "    return np.sign(pred)\n",
    "\n",
    "def plot_decision_boundary_forest(f, weights, X, y, fig_size=(7,7)):\n",
    "    plot_step = 0.02\n",
    "    x0_min, x0_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, plot_step), np.arange(x1_min, x1_max, plot_step))\n",
    "    yypred = forest_predict(f, weights, np.c_[xx0.ravel(),xx1.ravel()])\n",
    "    yypred = yypred.reshape(xx0.shape)\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.contourf(xx0, xx1, yypred, cmap=plt.cm.Paired)\n",
    "    y_pred = forest_predict(f, weights, X)\n",
    "    Xblue_good = X[np.equal(y,-1)*np.equal(y,y_pred)]\n",
    "    Xblue_bad  = X[np.equal(y,-1)*np.not_equal(y,y_pred)]\n",
    "    Xred_good  = X[np.equal(y,1)*np.equal(y,y_pred)]\n",
    "    Xred_bad   = X[np.equal(y,1)*np.not_equal(y,y_pred)]\n",
    "    plt.scatter(Xblue_good[:,0],Xblue_good[:,1],c='b',s=20)\n",
    "    plt.scatter(Xblue_bad[:,0],Xblue_bad[:,1],c='c',marker='v',s=20)\n",
    "    plt.scatter(Xred_good[:,0],Xred_good[:,1],c='r',s=20)\n",
    "    plt.scatter(Xred_bad[:,0],Xred_bad[:,1],c='m',marker='v',s=20)\n",
    "    plt.show()\n",
    "\n",
    "forest = [dt1]\n",
    "plot_decision_boundary_forest(forest,np.ones(1),X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement an AdaBoost algorithm with trees and visualize its decision boundary.<br>\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice**:<br>\n",
    "Using the elements introduced above, implement AdaBoost on a forest containing 100 trees, trained on the current dataset.<br>\n",
    "Display how the decision boundary evolves.<br>\n",
    "Plot on the same graph the evolution of the training and testing errors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/code1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "Nsteps = 100\n",
    "forest = list()\n",
    "sample_weights = np.ones(len(y))/len(y)\n",
    "tree_weights = np.zeros(Nsteps)\n",
    "single_tree_training_error = np.zeros(Nsteps)\n",
    "overall_training_error = np.zeros(Nsteps)\n",
    "generalization_error = np.zeros(Nsteps)\n",
    "for i in range(Nsteps):\n",
    "    # Train tree\n",
    "    dt = tree.DecisionTreeClassifier(criterion='entropy',max_depth=3)\n",
    "    dt.fit(X,y,sample_weight=sample_weights)\n",
    "    # Compute error\n",
    "    y_pred = dt.predict(X)\n",
    "    classif_error = sum(np.not_equal(y_pred, y)*sample_weights) / sum(sample_weights)\n",
    "    forest.append(dt)\n",
    "    # Get tree weight\n",
    "    alpha = .5*np.log((1-classif_error)/classif_error)\n",
    "    tree_weights[i] = alpha\n",
    "    # Update weights\n",
    "    sample_weights = sample_weights*np.exp(-alpha*y_pred*y)\n",
    "    sample_weights = sample_weights/sum(sample_weights)\n",
    "    # Plot and store data\n",
    "    #plot_decision_boundary_forest(forest, sample_weights, X, y)\n",
    "    single_tree_training_error[i] = classif_error\n",
    "    y_pred = forest_predict(forest, tree_weights, X)\n",
    "    overall = sum(np.not_equal(y_pred, y))/len(y)\n",
    "    overall_training_error[i] = overall\n",
    "    y_pred = forest_predict(forest, tree_weights, Xtest)\n",
    "    gen = sum(np.not_equal(y_pred, ytest))/len(ytest)\n",
    "    generalization_error[i] = gen\n",
    "    #print(\"Nb trees %d. Last tree error %.3g. Training error %.3g. Generalization error %.3g. Press Enter\"\n",
    "    #      %(len(forest),classif_error, overall, gen))\n",
    "    #input()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(single_tree_training_error,c='b')\n",
    "plt.plot(overall_training_error,c='r')\n",
    "plt.plot(generalization_error,c='g')\n",
    "plt.show()\n",
    "plot_decision_boundary_forest(forest, sample_weights, X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Question**:\n",
    "Is there a tendency to overfit?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a id=\"sec4\"></a> 4. AdaBoost in scikit-learn\n",
    "\n",
    "Scikit-learn provides an [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) meta-algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "boosted_forest = AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy',max_depth=3), n_estimators=100)\n",
    "boosted_forest.fit(X,y)\n",
    "\n",
    "plot_decision_boundary(boosted_forest,X,y)\n",
    "print(\"Training score:\", boosted_forest.score(X,y))\n",
    "print(\"Testing score: \", boosted_forest.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a id=\"sec5\"></a> 5. Gradient Boosting\n",
    "\n",
    "AdaBoost incrementally builds a committee of classifiers (or regressors) where each one tries to compensate the weaknesses of all the previous ones. This description makes it sound a lot like gradient descent.\n",
    "\n",
    "Let's write $f_k = \\{h_0,\\ldots,h_k\\}$ the committee predictor obtained at step $k$. $f_k$ is a point in $\\mathcal{H}$ and our goal is to find the function (the point) in $\\mathcal{H}$ that minimizes a certain loss function $L(f), \\ f\\in\\mathcal{H}$. Therefore, $h_{k+1}$ should point in the opposite direction of the gradient of $L$ with respect to functions $f$.\n",
    "\n",
    "This is precisely the idea of Gradient Boosting, that extends traditionnal Boosting to any differentiable loss function. Let's write this formally.\n",
    "\n",
    "Suppose we have a loss function $L(f(x),y)$ that quantifies how bad a predictor $f$ is, given the true data pair $(x,y)$. This could be a squared error, a cross-entropy, or any other loss function that compares $f(x)$ to the true $(x,y)$ data.\n",
    "\n",
    "The goal of a learning algorithm is to find\n",
    "$$f^* = \\arg\\min_{f\\in\\mathcal{H}} \\mathbb{E}_{(x,y)\\sim p(x,y)} \\left( L(f(x),y) \\right)$$\n",
    "\n",
    "To simplify the notation, we will write $\\mathbb{E}_{x,y}$ for $\\mathbb{E}_{(x,y)\\sim p(x,y)}$.\n",
    "\n",
    "So if we assume an current function $f_{k-1}$, one can perform gradient descent by writing \n",
    "$$f_{k} = f_{k-1} + \\alpha_{k} h_{k}$$\n",
    "Where\n",
    "$$h_{k} = \\nabla_f \\left[\\mathbb{E}_{x,y} \\left( L(f_{k-1}(x),y) \\right)\\right] = \\mathbb{E}_{x,y} \\left[ \\nabla_f L(f_{k-1}(x),y) \\right]$$\n",
    "And $\\alpha_{k}$ is found by line search\n",
    "$$\\alpha_{k} = \\arg\\min_\\alpha \\left[ \\mathbb{E}_{x,y} \\left[ L(f_{k-1}(x) + \\alpha h_{k}(x),y) \\right] \\right]$$\n",
    "\n",
    "Of course, the true value of $\\mathbb{E}_{x,y} \\left[ \\nabla_f L(f_k(x),y) \\right]$ is not accessible since it would require an infinite amount of data. But we can still approximate it using the training set:\n",
    "$$h_{k} = \\sum_{i} \\nabla_f L(f_{k-1}(x_i),y_i)$$\n",
    "And similarly\n",
    "$$\\alpha_{k} = \\arg\\min_\\alpha \\left[ \\sum_{i} L(f_{k-1}(x_i) + \\alpha h_{k}(x_i),y) \\right]$$\n",
    "\n",
    "Taking the gradient with respect to $f$ functions in $\\mathcal{H}$ is quite an abstract operation, so how do we choose the descent direction in practice? The key remark here is to notice that, for a given function $f$, although $f$ lives in a possibly infinite-dimensional function space, $f(x_i)$ lives in $\\mathbb{R}$ and is a descriptor of $f$ (around $x_i$). Thus, the function that would fit the training set $\\left\\{ \\left(x_i, \\left[\\frac{\\partial L(f(x_i), y_i)}{\\partial f(x_i)}\\right]_{f = f_{k-1}} \\right) \\right\\}$ is an approximate descent direction.\n",
    "\n",
    "The initial function $f_0$ is chosen to be a constant function, so:\n",
    "$$f_0 = \\arg\\min_\\gamma \\sum_i L(\\gamma, y_i)$$\n",
    "\n",
    "So the algorithm can be written:\n",
    "<div class=\"alert alert-success\">\n",
    "$f_0 = \\arg\\min_\\gamma \\sum_i L(\\gamma, y_i)$<br>\n",
    "For $k=1$ to $K$\n",
    "<ul>\n",
    "<li> Compute the pseudo-residuals\n",
    "$$r_i = \\left[ \\frac{\\partial L(f(x_i), y)}{\\partial f(x_i)} \\right]_{f = f_{k-1}}$$\n",
    "<li> Train $h_k$ to fit the dataset $\\left(x_i, r_i\\right)$\n",
    "<li> Find $\\alpha_k$ though line search\n",
    "$$\\alpha_{k} = \\arg\\min_\\alpha \\left[ \\sum_i L\\left(f_{k-1}(x_i) + \\alpha h_{k}(x_i),y_i\\right) \\right]$$\n",
    "<li> Update the model\n",
    "$$f_k = f_{k-1} + \\alpha_k h_k$$\n",
    "</ul>\n",
    "Return $f_K$\n",
    "</div>\n",
    "\n",
    "AdaBoost is actually a Gradient Boosting procedure.\n",
    "\n",
    "When $\\mathcal{H}$ is the set of regression or classification trees, Gradient Boosting is called **Gradient Tree Boosting**. Several extensions (regularization, local step-sizes, etc.) are possible in that case.\n",
    "\n",
    "The most well-known Gradient Boosting library is called [XGBoost](https://github.com/dmlc/xgboost). It is efficient and quite flexible. It has be used to win several Data Science competitions. But Scikit-Learn also provides a decent implementation that we will use below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a id=\"sec5\"></a> 5. Gradient Boosting (bis)\n",
    "\n",
    "AdaBoost incrementally builds a committee of classifiers (or regressors) where each one tries to compensate the weaknesses of all the previous ones. This description makes it sound a lot like gradient descent.\n",
    "\n",
    "Let's write $f_k = \\{h_0,\\ldots,h_k\\}$ the committee predictor obtained at step $k$. $f_k$ is a point in $\\mathcal{H}$ and our goal is to find the function (the point) in $\\mathcal{H}$ that minimizes a certain loss function $L(f), \\ f\\in\\mathcal{H}$. Therefore, $h_{k+1}$ should point in the opposite direction of the gradient of $L$ with respect to functions $f$.\n",
    "\n",
    "This is precisely the idea of Gradient Boosting, that extends traditionnal Boosting to any differentiable loss function. Let's write this formally.\n",
    "\n",
    "Suppose we have a loss function $L(f(x),y)$ that quantifies how bad a predictor $f$ is, given the true data pair $(x,y)$. This could be a squared error, a cross-entropy, or any other loss function that compares $f(x)$ to the true $(x,y)$ data.\n",
    "\n",
    "Our goal is to find $f_{k+1}$ so that it compensates the error of $f_k$ made at each input $x$. In other words, we would like to move the output of the predictor $f_k$ so that the loss reduces at each $x$. Thus, we will look at the variation of the loss with respect to the variation of the prediction of $f_k$. Formally, this is the derivative of the loss $L(f_k(x), y)$ with respect to the prediction $f_k(x)$.\n",
    "$$\\frac{\\partial L(f_k(x), y)}{\\partial f_k(x)}$$\n",
    "\n",
    "As in gradient descent, by adding the new weak learner $h_k$, we would like to move in the opposite direction (steepest descent reducing the loss) from a certain quantity $\\alpha_k$:\n",
    "$$f_{k+1} = f_{k} - \\alpha_{k} h_{k}$$\n",
    "\n",
    "Practically, we do not have an estimate of $\\frac{\\partial L(f_k(x), y)}{\\partial f_k(x)}$ for any $x$, but only on the data set $\\{(x_i, y_i)\\}_i$. This yields the following desired value of $h_{k}$ for all $(x_i, y_i)$: $\\frac{\\partial L(f_k(x_i), y_i)}{\\partial f_k(x_i)}$.\n",
    "Thus, we will seek to fit $h_k$ to the following training set:\n",
    "$$\\left\\{ \\left(x_i, \\frac{\\partial L(f(x_i), y_i)}{\\partial f(x_i)} \\right) \\right\\}_i$$\n",
    "\n",
    "This result is the essence of the gradient boosting method. The latter training set can be seen as the set of points that compensates the error (or reduces the loss) of $f_k$ at each $x_i$.\n",
    "\n",
    "Notice that for gradient boosting to be applied, the only requirement is for the loss function to be differentiable. The weak learners themselves could be any functions.\n",
    "\n",
    "The algorithm can be written as follows:\n",
    "<div class=\"alert alert-success\">\n",
    "$f_0 = \\arg\\min_\\gamma \\sum_i L(\\gamma, y_i)$<br>\n",
    "For $k=1$ to $K$\n",
    "<ul>\n",
    "<li> Compute the pseudo-residuals\n",
    "$$r_i = \\left[ \\frac{\\partial L(f(x_i), y)}{\\partial f(x_i)} \\right]_{f = f_{k-1}}$$\n",
    "<li> Train $h_k$ to fit the dataset $\\left(x_i, r_i\\right)$\n",
    "<li> Find $\\alpha_k$ though line search\n",
    "$$\\alpha_{k} = \\arg\\min_\\alpha \\left[ \\sum_i L\\left(f_{k-1}(x_i) + \\alpha h_{k}(x_i),y_i\\right) \\right]$$\n",
    "<li> Update the model\n",
    "$$f_k = f_{k-1} + \\alpha_k h_k$$\n",
    "</ul>\n",
    "Return $f_K$\n",
    "</div>\n",
    "\n",
    "AdaBoost is actually a Gradient Boosting procedure.\n",
    "\n",
    "When $\\mathcal{H}$ is the set of regression or classification trees, Gradient Boosting is called **Gradient Tree Boosting**. Several extensions (regularization, local step-sizes, etc.) are possible in that case.\n",
    "\n",
    "The most well-known Gradient Boosting library is called [XGBoost](https://github.com/dmlc/xgboost). It is efficient and quite flexible. It has be used to win several Data Science competitions. But Scikit-Learn also provides a decent implementation that we will use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gtb = GradientBoostingClassifier(n_estimators=100)\n",
    "gtb.fit(X,y)\n",
    "\n",
    "plot_decision_boundary(gtb,X,y)\n",
    "print(\"Training score:\", gtb.score(X,y))\n",
    "print(\"Testing score: \", gtb.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward stagewise boosting (Algorithm 10.2) is also a very greedy strategy.\n",
    "At each step the solution tree is the one that maximally reduces (10.29)# <a id=\"sec6\"></a> 6. Examples\n",
    "\n",
    "If you have done the previous notebooks, you are used to these two examples now.\n",
    "\n",
    "## <a id=\"sec6-1\"></a>6.1 Spam or ham?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append('../2 - Text data preprocessing')\n",
    "import load_spam\n",
    "spam_data = load_spam.spam_data_loader()\n",
    "spam_data.load_data()\n",
    "\n",
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_ada = AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy',max_depth=3), n_estimators=100)\n",
    "spam_ada.fit(Xtrain,ytrain)\n",
    "print(\"score:\", spam_ada.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_gtb = GradientBoostingClassifier(n_estimators=100)\n",
    "spam_gtb.fit(Xtrain,ytrain)\n",
    "print(\"score:\", spam_gtb.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = spam_data.split(2000, feat='wordcount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_ada = AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy',max_depth=3), n_estimators=100)\n",
    "spam_ada.fit(Xtrain,ytrain)\n",
    "print(\"score:\", spam_ada.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_gtb = GradientBoostingClassifier(n_estimators=100)\n",
    "spam_gtb.fit(Xtrain,ytrain)\n",
    "print(\"score:\", spam_gtb.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec6-2\"></a> 6.2 NIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "#print(digits.data.shape)\n",
    "#print(digits.images.shape)\n",
    "#print(digits.target.shape)\n",
    "#print(digits.target_names)\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "Xtrain,Xtest = np.split(X,[1000])\n",
    "ytrain,ytest = np.split(y,[1000])\n",
    "\n",
    "#print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def shuffle_and_split(X,y,n):\n",
    "    X0,y0 = shuffle(X,y)\n",
    "    Xtrain,Xtest = np.split(X0,[n])\n",
    "    ytrain,ytest = np.split(y0,[n])\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "digits_ada = AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy',max_depth=3), n_estimators=100)\n",
    "digits_ada.fit(Xtrain,ytrain)\n",
    "prediction = digits_ada.predict(Xtest)\n",
    "#print(\"Training error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest))\n",
    "print(\"Generalization error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest) )\n",
    "print(\"Generalization score:\", digits_ada.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_gtb = GradientBoostingClassifier(n_estimators=100)\n",
    "digits_gtb.fit(Xtrain,ytrain)\n",
    "prediction = digits_ada.predict(Xtest)\n",
    "#print(\"Training error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest))\n",
    "print(\"Generalization error:\", np.sum(np.not_equal(prediction,ytest))/len(ytest) )\n",
    "print(\"Generalization score:\", digits_gtb.score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "232px",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
